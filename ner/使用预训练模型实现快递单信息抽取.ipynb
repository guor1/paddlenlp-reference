{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc5d196-29c4-4067-9579-3bd6b1e41e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffba29c1-d6ea-4f6e-b0ab-46087b473d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:00<00:00, 2731.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\data'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a62282-70a4-41f1-a7da-1d75c2b898c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=(\n",
    "        './data/train.txt', './data/dev.txt', './data/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5bee04d-089e-4f1f-9b73-11c010c2b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        key = line.strip('\\n')\n",
    "        vocab[key] = i\n",
    "        i+=1\n",
    "    return vocab\n",
    "\n",
    "def convert_example(example, tokenizer, label_vocab):\n",
    "    tokens, labels = example\n",
    "    tokenized_input = tokenizer(\n",
    "        tokens, return_length=True, is_split_into_words=True)\n",
    "    # Token '[CLS]' and '[SEP]' will get label 'O'\n",
    "    labels = ['O'] + labels + ['O']\n",
    "    tokenized_input['labels'] = [label_vocab[x] for x in labels]\n",
    "    return tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['seq_len'], tokenized_input['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67a79a9b-67cd-4dc0-b090-d243cf2d84bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-07-30 19:34:31,825] [    INFO]\u001b[0m - Already cached C:\\Users\\guor\\.paddlenlp\\models\\ernie-3.0-medium-zh\\ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-07-30 19:34:31,859] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\guor\\.paddlenlp\\models\\ernie-3.0-medium-zh\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-07-30 19:34:31,862] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\guor\\.paddlenlp\\models\\ernie-3.0-medium-zh\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 208, 515, 515, 249, 540, 249, 540, 540, 540, 589, 589, 803, 838, 2914, 1222, 1734, 244, 368, 797, 99, 32, 863, 308, 457, 2778, 484, 167, 436, 930, 192, 233, 634, 99, 213, 40, 317, 540, 256, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 40, [12, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 4, 5, 5, 6, 7, 7, 8, 9, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "label_vocab = load_dict('./data/tag.dic')\n",
    "MODEL_NAME = \"ernie-3.0-medium-zh\"\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd203d76-21e6-4226-9d1c-a00fb889940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22077947-614a-4360-8978-f028bd4b00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ErnieForTokenClassification.from_pretrained(MODEL_NAME, num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2564750-9175-40c2-8a8b-6271f698fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4b74d-e68b-4f3d-b008-4d528e557728",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(10):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "                './ernie_result/model_%d.pdparams' % step)\n",
    "model.save_pretrained('./pretrained_model')\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
