{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9fc1ab-b7d6-4098-8656-b8cc5f56950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guorui1\\.conda\\envs\\pdd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\guorui1\\.conda\\envs\\pdd\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Vocab, JiebaTokenizer\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "from paddlenlp.seq2vec import LSTMEncoder\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Stack, Pad, Dict, Tuple\n",
    "from paddlenlp.datasets import MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7760ca72-7ad0-46ee-ae6e-ec7634669c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-05-30 18:32:01,747] [    INFO]\u001b[0m - Loading token embedding...\u001b[0m\n",
      "\u001b[32m[2024-05-30 18:32:10,930] [    INFO]\u001b[0m - Finish loading embedding vector.\u001b[0m\n",
      "\u001b[32m[2024-05-30 18:32:10,930] [    INFO]\u001b[0m - Token Embedding info:             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Shape :[635965, 300]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 初始化TokenEmbedding，预训练embedding未下载时会自动下载并加载数据\n",
    "# 内置的预训练词向量 paddlenlp.embeddings.list_embedding_name()\n",
    "token_embedder = TokenEmbedding()\n",
    "tokenizer = JiebaTokenizer(token_embedder.vocab)\n",
    "\n",
    "emb_dim = token_embedder.embedding_dim\n",
    "vocab_size = len(token_embedder.vocab)\n",
    "pad_token_id = token_embedder.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72490bec-7b89-45d3-a070-d378f54f7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, dev_ds, test_ds = load_dataset('chnsenticorp', splits=['train', 'dev', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859cb5d6-d6af-4434-9a44-f8c774b4abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x1c1f10b9450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 256\n",
    "def convert_example(example, tokenizer):\n",
    "    input_ids = tokenizer.encode(example['text'])\n",
    "    seq_len = len(input_ids)\n",
    "    input_ids = input_ids[:max_len]\n",
    "    input_ids = input_ids + [pad_token_id] * (max_len - len(input_ids))\n",
    "    return input_ids, example['label'], seq_len\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "train_ds.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a11b2c-f3c7-4899-9be6-dd2d5003c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "batchify_fn = lambda samples, fn=Tuple([\n",
    "    Stack(dtype=\"int64\"),\n",
    "    Stack(dtype=\"int64\"),\n",
    "    Stack(dtype=\"int64\")\n",
    "]): fn(samples)\n",
    "batch_sampler = BatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(dataset=train_ds, batch_sampler=batch_sampler, collate_fn=batchify_fn, return_list=True)\n",
    "items = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ae808b-d576-415d-b3d4-43d3b4b4258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 256, 300]\n",
      "[16, 128]\n"
     ]
    }
   ],
   "source": [
    "lstm_encoder = LSTMEncoder(emb_dim, 128)\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    input_ids, labels, seq_lens = batch\n",
    "    # 输入shape = [batch_size, max_seq_len]\n",
    "    # 输出shape=[batch_size, max_seq_len, emb_dim]\n",
    "    embedded_text = token_embedder(input_ids)\n",
    "    text_repr = lstm_encoder(embedded_text, sequence_length=seq_lens)\n",
    "    print(text_repr.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f5a6424-eee9-428b-ba6c-7a80d4287a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "        # 首先将输入word id 查表后映射成 word embedding\n",
    "        self.embedder = token_embedder\n",
    "        # 将word embedding经过LSTMEncoder变换到文本语义表征空间中\n",
    "        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        # LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "\n",
    "        # 最后的分类器\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "\n",
    "\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        \n",
    "        # probs 分类概率值\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        return probs\n",
    "\n",
    "model= LSTMModel(\n",
    "    vocab_size = len(token_embedder.vocab),\n",
    "    num_classes = len(train_ds.label_list),\n",
    "    emb_dim = token_embedder.embedding_dim,\n",
    "    direction='bidirectional',\n",
    "    padding_idx = token_embedder.vocab['[PAD]'])\n",
    "model = paddle.Model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
