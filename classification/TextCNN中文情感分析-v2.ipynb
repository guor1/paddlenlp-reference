{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa51c012-83db-499e-bcf4-c535b1a102d4",
   "metadata": {},
   "source": [
    "# 基于卷积神经网络的文本分类\n",
    "\n",
    "1. 论文 https://arxiv.org/pdf/1408.5882.pdf\n",
    "2. 参考实现 https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62301162-ede4-4337-ad3a-a6631b25c07f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install paddlenlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9822c9f0-658a-486b-875e-af4380b50f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:45.110108Z",
     "iopub.status.busy": "2023-07-30T08:51:45.109401Z",
     "iopub.status.idle": "2023-07-30T08:51:50.181240Z",
     "shell.execute_reply": "2023-07-30T08:51:50.180302Z",
     "shell.execute_reply.started": "2023-07-30T08:51:45.110080Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n",
      "  from .autonotebook import tqdm as notebook_tqdm\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0730 16:51:47.590281   622 interpretercore.cc:237] New Executor is Running.\r\n",
      "W0730 16:51:47.591131   622 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.6\r\n",
      "W0730 16:51:47.597573   622 gpu_resources.cc:149] device: 0, cuDNN Version: 8.4.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddlePaddle works well on 1 GPU.\r\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\r\n",
      "自然语言相关数据集： ['Conll05st', 'Imdb', 'Imikolov', 'Movielens', 'UCIHousing', 'WMT14', 'WMT16', 'ViterbiDecoder', 'viterbi_decode']\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0730 16:51:50.169422   622 interpreter_util.cc:518] Standalone Executor is Used.\r\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import pandas as pd\n",
    "from paddle.utils import run_check\n",
    "from paddle import nn\n",
    "import paddlenlp\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp import datasets, transformers\n",
    "from visualdl import LogWriter\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from paddlenlp.data import Vocab\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "run_check()\n",
    "print('自然语言相关数据集：', paddle.text.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988be30c-5426-49bb-bb69-616fc022c207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:50.388093Z",
     "iopub.status.busy": "2023-07-30T08:51:50.387811Z",
     "iopub.status.idle": "2023-07-30T08:51:50.567609Z",
     "shell.execute_reply": "2023-07-30T08:51:50.566921Z",
     "shell.execute_reply.started": "2023-07-30T08:51:50.388073Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "train_ds, dev_ds, test_ds = paddlenlp.datasets.load_dataset('chnsenticorp', splits=['train', 'dev', 'test'])\n",
    "num_labels = len(train_ds.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9738c3d3-4d0d-49f8-87f4-748463193e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:50.569370Z",
     "iopub.status.busy": "2023-07-30T08:51:50.569078Z",
     "iopub.status.idle": "2023-07-30T08:51:56.387412Z",
     "shell.execute_reply": "2023-07-30T08:51:56.386448Z",
     "shell.execute_reply.started": "2023-07-30T08:51:50.569350Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\r\n",
      "[2023-07-30 16:51:50,571] [   DEBUG] __init__.py:113 - Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "[2023-07-30 16:51:50,572] [   DEBUG] __init__.py:132 - Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.724 seconds.\r\n",
      "[2023-07-30 16:51:51,296] [   DEBUG] __init__.py:164 - Loading model cost 0.724 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "[2023-07-30 16:51:51,297] [   DEBUG] __init__.py:166 - Prefix dict has been built successfully.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  8253\r\n"
     ]
    }
   ],
   "source": [
    "#构建词典\n",
    "words = (word for item in train_ds for word in jieba.lcut(item['text'], use_paddle=True))\n",
    "words_counter = Counter(words)\n",
    "vocab = Vocab(words_counter, min_freq=5, unk_token='[UNK]', pad_token='[PAD]')\n",
    "json_str = vocab.to_json(\"./vocab.json\")\n",
    "#嵌入字典的大小\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318cf7a5-b7dd-4cfa-9c2c-0a7fa00f7d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:56.389110Z",
     "iopub.status.busy": "2023-07-30T08:51:56.388749Z",
     "iopub.status.idle": "2023-07-30T08:51:56.394949Z",
     "shell.execute_reply": "2023-07-30T08:51:56.394368Z",
     "shell.execute_reply.started": "2023-07-30T08:51:56.389083Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_tokens: ['选择', '珠江', '花园', '的', '原因', '就是', '方便', '，', '有', '电动', '扶梯', '直接', '到达', '海边', '，', '周围', '餐馆', '、', '食廊', '、', '商场', '、', '超市', '、', '摊位', '一应俱全', '。', '酒店', '装修', '一般', '，', '但', '还', '算', '整洁', '。', ' ', '泳池', '在', '大堂', '的', '屋顶', '，', '因此', '很小', '，', '不过', '女儿', '倒', '是', '喜欢', '。', ' ', '包', '的', '早餐', '是', '西式', '的', '，', '还', '算', '丰富', '。', ' ', '服务', '吗', '，', '一般']\r\n",
      "_indices: [202, 1, 1635, 3, 404, 40, 107, 2, 17, 1, 1, 364, 982, 1915, 2, 675, 2979, 27, 1, 27, 1525, 27, 1420, 27, 1, 4366, 4, 14, 288, 94, 2, 45, 21, 244, 1062, 4, 6, 3362, 13, 310, 3, 7572, 2, 1047, 482, 2, 95, 374, 466, 7, 58, 4, 6, 543, 3, 106, 7, 3218, 3, 2, 21, 244, 534, 4, 6, 44, 343, 2, 94]\r\n"
     ]
    }
   ],
   "source": [
    "_tokens = jieba.lcut(train_ds[0]['text'], use_paddle=True)\n",
    "_indices = vocab.to_indices(_tokens)\n",
    "print(\"_tokens:\", _tokens)\n",
    "print(\"_indices:\", _indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47f1c1f-3f18-4184-8877-e24a76820983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:56.396049Z",
     "iopub.status.busy": "2023-07-30T08:51:56.395785Z",
     "iopub.status.idle": "2023-07-30T08:51:56.403756Z",
     "shell.execute_reply": "2023-07-30T08:51:56.403224Z",
     "shell.execute_reply.started": "2023-07-30T08:51:56.396026Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x7ff1387f60e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 700\n",
    "pad_token_id = vocab.to_indices(\"[PAD]\")\n",
    "def convert_example(example, vocab):\n",
    "    _tokens = jieba.lcut(example['text'], use_paddle=True)\n",
    "    input_ids = vocab.to_indices(_tokens)\n",
    "    input_ids = input_ids[:max_len]\n",
    "    input_ids = input_ids + [pad_token_id] * (max_len - len(input_ids))\n",
    "    return input_ids, example['label']\n",
    "\n",
    "trans_func = partial(convert_example, vocab=vocab)\n",
    "train_ds.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444be668-757b-4b78-9f67-c75939d1def1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:56.405009Z",
     "iopub.status.busy": "2023-07-30T08:51:56.404638Z",
     "iopub.status.idle": "2023-07-30T08:51:56.941727Z",
     "shell.execute_reply": "2023-07-30T08:51:56.939209Z",
     "shell.execute_reply.started": "2023-07-30T08:51:56.404990Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "batchify_fn = lambda samples, fn=Tuple([\n",
    "    Stack(dtype=\"int64\"),\n",
    "    Stack(dtype=\"int64\")\n",
    "]): fn(samples)\n",
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = paddle.io.DataLoader(dataset=train_ds, batch_sampler=batch_sampler, collate_fn=batchify_fn, return_list=True)\n",
    "items = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976a5fb3-ad54-4deb-a3fb-5bfe2162bc39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:56.946842Z",
     "iopub.status.busy": "2023-07-30T08:51:56.942840Z",
     "iopub.status.idle": "2023-07-30T08:51:56.955381Z",
     "shell.execute_reply": "2023-07-30T08:51:56.949041Z",
     "shell.execute_reply.started": "2023-07-30T08:51:56.946814Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lens = []\n",
    "# for input_ids, label in train_ds:\n",
    "#     lens.append(len(input_ids))\n",
    "\n",
    "# df = pd.DataFrame(lens, columns=['len'])\n",
    "# df.describe(percentiles=[0.5, 0.999]) #700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f4f7f0-769f-4452-84ed-57d8adcf9141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:57.001293Z",
     "iopub.status.busy": "2023-07-30T08:51:57.001048Z",
     "iopub.status.idle": "2023-07-30T08:51:57.083719Z",
     "shell.execute_reply": "2023-07-30T08:51:57.082891Z",
     "shell.execute_reply.started": "2023-07-30T08:51:57.001274Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as nlp\n",
    "\n",
    "class TextCNN(nn.Layer):\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                num_classes,\n",
    "                emb_dim=128,\n",
    "                padding_idx=0,\n",
    "                num_filter=128,\n",
    "                ngram_filter_sizes=(3, 4, 5),\n",
    "                fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
    "        self.convs = paddle.nn.LayerList(\n",
    "            [nn.Conv2D(in_channels=1, out_channels=num_filter, kernel_size=(kernel_size, emb_dim)) for kernel_size in ngram_filter_sizes]\n",
    "        )\n",
    "        self.activation = nn.Tanh()\n",
    "        maxpool_output_dim = num_filter * len(ngram_filter_sizes)\n",
    "        self.output_layer = nn.Linear(maxpool_output_dim, num_classes)\n",
    "        \n",
    "\n",
    "    def encoder(self, embeddings):\n",
    "        embeddings = embeddings.unsqueeze(1)\n",
    "        convs_out = [self.activation(conv(embeddings)).squeeze(3) for conv in self.convs]\n",
    "        maxpool_out = [F.adaptive_max_pool1d(t, output_size=1).squeeze(2) for t in convs_out]\n",
    "        return paddle.concat(maxpool_out, axis=1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embeddings = self.embedder(input_ids)\n",
    "        encoding = self.encoder(embeddings)\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(encoding)\n",
    "        return logits\n",
    "\n",
    "model = TextCNN(vocab_size=vocab_size, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2852c049-599e-42f1-9e22-c41c7eb6902f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:51:57.194216Z",
     "iopub.status.busy": "2023-07-30T08:51:57.193971Z",
     "iopub.status.idle": "2023-07-30T08:52:47.737515Z",
     "shell.execute_reply": "2023-07-30T08:52:47.736628Z",
     "shell.execute_reply.started": "2023-07-30T08:51:57.194197Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 10: loss:0.68145, acc:0.51953\r\n",
      "epoch 1, step 20: loss:0.64644, acc:0.63281\r\n",
      "epoch 1, step 30: loss:0.57183, acc:0.67526\r\n",
      "epoch 1, step 40: loss:0.51998, acc:0.69824\r\n",
      "epoch 1, step 50: loss:0.43974, acc:0.71625\r\n",
      "epoch 1, step 60: loss:0.43186, acc:0.73281\r\n",
      "epoch 1, step 70: loss:0.41584, acc:0.74844\r\n",
      "epoch 2, step 5: loss:0.26225, acc:0.76572\r\n",
      "epoch 2, step 15: loss:0.31434, acc:0.78047\r\n",
      "epoch 2, step 25: loss:0.17671, acc:0.79273\r\n",
      "epoch 2, step 35: loss:0.24877, acc:0.80355\r\n",
      "epoch 2, step 45: loss:0.25345, acc:0.81289\r\n",
      "epoch 2, step 55: loss:0.22099, acc:0.82073\r\n",
      "epoch 2, step 65: loss:0.20602, acc:0.82785\r\n",
      "epoch 2, step 75: loss:0.22985, acc:0.83427\r\n",
      "epoch 3, step 10: loss:0.11934, acc:0.84272\r\n",
      "epoch 3, step 20: loss:0.08393, acc:0.85014\r\n",
      "epoch 3, step 30: loss:0.11284, acc:0.85660\r\n",
      "epoch 3, step 40: loss:0.11860, acc:0.86262\r\n",
      "epoch 3, step 50: loss:0.05977, acc:0.86805\r\n",
      "epoch 3, step 60: loss:0.13798, acc:0.87206\r\n",
      "epoch 3, step 70: loss:0.11340, acc:0.87610\r\n",
      "epoch 4, step 5: loss:0.03653, acc:0.88047\r\n",
      "epoch 4, step 15: loss:0.05623, acc:0.88480\r\n",
      "epoch 4, step 25: loss:0.03494, acc:0.88894\r\n",
      "epoch 4, step 35: loss:0.03962, acc:0.89282\r\n",
      "epoch 4, step 45: loss:0.03308, acc:0.89630\r\n",
      "epoch 4, step 55: loss:0.05127, acc:0.89930\r\n",
      "epoch 4, step 65: loss:0.04703, acc:0.90210\r\n",
      "epoch 4, step 75: loss:0.02754, acc:0.90492\r\n",
      "epoch 5, step 10: loss:0.06735, acc:0.90781\r\n",
      "epoch 5, step 20: loss:0.05410, acc:0.91042\r\n",
      "epoch 5, step 30: loss:0.01961, acc:0.91304\r\n",
      "epoch 5, step 40: loss:0.01210, acc:0.91535\r\n",
      "epoch 5, step 50: loss:0.10147, acc:0.91748\r\n",
      "epoch 5, step 60: loss:0.09949, acc:0.91949\r\n",
      "epoch 5, step 70: loss:0.01437, acc:0.92145\r\n",
      "epoch 6, step 5: loss:0.02988, acc:0.92340\r\n",
      "epoch 6, step 15: loss:0.01196, acc:0.92524\r\n",
      "epoch 6, step 25: loss:0.01891, acc:0.92691\r\n",
      "epoch 6, step 35: loss:0.05033, acc:0.92854\r\n",
      "epoch 6, step 45: loss:0.00676, acc:0.93013\r\n",
      "epoch 6, step 55: loss:0.04184, acc:0.93167\r\n",
      "epoch 6, step 65: loss:0.02056, acc:0.93313\r\n",
      "epoch 6, step 75: loss:0.09183, acc:0.93434\r\n",
      "epoch 7, step 10: loss:0.02048, acc:0.93570\r\n",
      "epoch 7, step 20: loss:0.05830, acc:0.93700\r\n",
      "epoch 7, step 30: loss:0.04515, acc:0.93817\r\n",
      "epoch 7, step 40: loss:0.00574, acc:0.93932\r\n",
      "epoch 7, step 50: loss:0.00870, acc:0.94042\r\n",
      "epoch 7, step 60: loss:0.00488, acc:0.94147\r\n",
      "epoch 7, step 70: loss:0.00382, acc:0.94247\r\n",
      "epoch 8, step 5: loss:0.00501, acc:0.94347\r\n",
      "epoch 8, step 15: loss:0.00507, acc:0.94444\r\n",
      "epoch 8, step 25: loss:0.00517, acc:0.94538\r\n",
      "epoch 8, step 35: loss:0.00399, acc:0.94630\r\n",
      "epoch 8, step 45: loss:0.03656, acc:0.94715\r\n",
      "epoch 8, step 55: loss:0.01009, acc:0.94797\r\n",
      "epoch 8, step 65: loss:0.00457, acc:0.94873\r\n",
      "epoch 8, step 75: loss:0.02627, acc:0.94951\r\n",
      "epoch 9, step 10: loss:0.00609, acc:0.95031\r\n",
      "epoch 9, step 20: loss:0.03503, acc:0.95105\r\n",
      "epoch 9, step 30: loss:0.00242, acc:0.95176\r\n",
      "epoch 9, step 40: loss:0.03426, acc:0.95238\r\n",
      "epoch 9, step 50: loss:0.00365, acc:0.95303\r\n",
      "epoch 9, step 60: loss:0.00506, acc:0.95367\r\n",
      "epoch 9, step 70: loss:0.01902, acc:0.95426\r\n",
      "epoch 10, step 5: loss:0.00248, acc:0.95489\r\n",
      "epoch 10, step 15: loss:0.00274, acc:0.95553\r\n",
      "epoch 10, step 25: loss:0.00183, acc:0.95614\r\n",
      "epoch 10, step 35: loss:0.02307, acc:0.95664\r\n",
      "epoch 10, step 45: loss:0.00178, acc:0.95717\r\n",
      "epoch 10, step 55: loss:0.00319, acc:0.95771\r\n",
      "epoch 10, step 65: loss:0.00304, acc:0.95819\r\n",
      "epoch 10, step 75: loss:0.03566, acc:0.95866\r\n"
     ]
    }
   ],
   "source": [
    "# 定义 optimizer 优化器\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=0.001, parameters=model.parameters())\n",
    "# 定义 loss\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "metric = paddle.metric.Accuracy()\n",
    "# 训练\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "with LogWriter(logdir=\"./logs\") as writer:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_dataloader, start=1):\n",
    "            input_ids, labels = batch\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            # 预测分类概率\n",
    "            correct = metric.compute(logits, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            writer.add_scalar(tag=\"acc\", step=global_step, value=acc)\n",
    "            # 向记录器添加一个tag为`loss`的数据\n",
    "            writer.add_scalar(tag=\"loss\", step=global_step, value=loss)\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0:\n",
    "                print(\"epoch %d, step %d: loss:%.5f, acc:%.5f\" % (epoch, step, loss, acc))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be9c9e34-7dda-49e5-974d-7c1fd8caa54a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T08:57:28.516214Z",
     "iopub.status.busy": "2023-07-30T08:57:28.515498Z",
     "iopub.status.idle": "2023-07-30T08:57:28.750154Z",
     "shell.execute_reply": "2023-07-30T08:57:28.698679Z",
     "shell.execute_reply.started": "2023-07-30T08:57:28.516189Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 1.0\r\n"
     ]
    }
   ],
   "source": [
    "input_ids, labels = next(iter(train_dataloader))\n",
    "logits = model(input_ids)\n",
    "pred = paddle.argmax(logits, axis=-1)\n",
    "print(\"f1_score:\", f1_score(labels, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
