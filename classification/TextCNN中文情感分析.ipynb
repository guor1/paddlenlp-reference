{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa51c012-83db-499e-bcf4-c535b1a102d4",
   "metadata": {},
   "source": [
    "# 基于卷积神经网络的文本分类\n",
    "\n",
    "1. 论文 https://arxiv.org/pdf/1408.5882.pdf\n",
    "2. 参考实现 https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62301162-ede4-4337-ad3a-a6631b25c07f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install paddlenlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9822c9f0-658a-486b-875e-af4380b50f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:26:42.990474Z",
     "iopub.status.busy": "2023-07-30T07:26:42.989911Z",
     "iopub.status.idle": "2023-07-30T07:26:43.018081Z",
     "shell.execute_reply": "2023-07-30T07:26:43.017366Z",
     "shell.execute_reply.started": "2023-07-30T07:26:42.990441Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \r\n",
      "PaddlePaddle works well on 1 GPU.\r\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\r\n",
      "自然语言相关数据集： ['Conll05st', 'Imdb', 'Imikolov', 'Movielens', 'UCIHousing', 'WMT14', 'WMT16', 'ViterbiDecoder', 'viterbi_decode']\r\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import pandas as pd\n",
    "from paddle.utils import run_check\n",
    "from paddle import nn\n",
    "import paddlenlp\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp import datasets, transformers\n",
    "from visualdl import LogWriter\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from paddlenlp.data import Vocab\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "run_check()\n",
    "print('自然语言相关数据集：', paddle.text.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988be30c-5426-49bb-bb69-616fc022c207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:19:58.216797Z",
     "iopub.status.busy": "2023-07-30T07:19:58.216322Z",
     "iopub.status.idle": "2023-07-30T07:19:58.425214Z",
     "shell.execute_reply": "2023-07-30T07:19:58.424459Z",
     "shell.execute_reply.started": "2023-07-30T07:19:58.216775Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "train_ds, dev_ds, test_ds = paddlenlp.datasets.load_dataset('chnsenticorp', splits=['train', 'dev', 'test'])\n",
    "num_labels = len(train_ds.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9738c3d3-4d0d-49f8-87f4-748463193e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:19:58.427249Z",
     "iopub.status.busy": "2023-07-30T07:19:58.426738Z",
     "iopub.status.idle": "2023-07-30T07:20:04.188559Z",
     "shell.execute_reply": "2023-07-30T07:20:04.186115Z",
     "shell.execute_reply.started": "2023-07-30T07:19:58.427225Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\r\n",
      "[2023-07-30 15:19:58,429] [   DEBUG] __init__.py:113 - Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "[2023-07-30 15:19:58,430] [   DEBUG] __init__.py:132 - Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.730 seconds.\r\n",
      "[2023-07-30 15:19:59,160] [   DEBUG] __init__.py:164 - Loading model cost 0.730 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "[2023-07-30 15:19:59,162] [   DEBUG] __init__.py:166 - Prefix dict has been built successfully.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  8253\r\n"
     ]
    }
   ],
   "source": [
    "#构建词典\n",
    "words = (word for item in train_ds for word in jieba.lcut(item['text'], use_paddle=True))\n",
    "words_counter = Counter(words)\n",
    "vocab = Vocab(words_counter, min_freq=5, unk_token='[UNK]', pad_token='[PAD]')\n",
    "json_str = vocab.to_json(\"./vocab.json\")\n",
    "#嵌入字典的大小\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318cf7a5-b7dd-4cfa-9c2c-0a7fa00f7d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:20:04.190492Z",
     "iopub.status.busy": "2023-07-30T07:20:04.190210Z",
     "iopub.status.idle": "2023-07-30T07:20:04.200148Z",
     "shell.execute_reply": "2023-07-30T07:20:04.198184Z",
     "shell.execute_reply.started": "2023-07-30T07:20:04.190471Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_tokens: ['选择', '珠江', '花园', '的', '原因', '就是', '方便', '，', '有', '电动', '扶梯', '直接', '到达', '海边', '，', '周围', '餐馆', '、', '食廊', '、', '商场', '、', '超市', '、', '摊位', '一应俱全', '。', '酒店', '装修', '一般', '，', '但', '还', '算', '整洁', '。', ' ', '泳池', '在', '大堂', '的', '屋顶', '，', '因此', '很小', '，', '不过', '女儿', '倒', '是', '喜欢', '。', ' ', '包', '的', '早餐', '是', '西式', '的', '，', '还', '算', '丰富', '。', ' ', '服务', '吗', '，', '一般']\r\n",
      "_indices: [202, 1, 1635, 3, 404, 40, 107, 2, 17, 1, 1, 364, 982, 1915, 2, 675, 2979, 27, 1, 27, 1525, 27, 1420, 27, 1, 4366, 4, 14, 288, 94, 2, 45, 21, 244, 1062, 4, 6, 3362, 13, 310, 3, 7572, 2, 1047, 482, 2, 95, 374, 466, 7, 58, 4, 6, 543, 3, 106, 7, 3218, 3, 2, 21, 244, 534, 4, 6, 44, 343, 2, 94]\r\n"
     ]
    }
   ],
   "source": [
    "_tokens = jieba.lcut(train_ds[0]['text'], use_paddle=True)\n",
    "_indices = vocab.to_indices(_tokens)\n",
    "print(\"_tokens:\", _tokens)\n",
    "print(\"_indices:\", _indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47f1c1f-3f18-4184-8877-e24a76820983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:20:04.201646Z",
     "iopub.status.busy": "2023-07-30T07:20:04.201426Z",
     "iopub.status.idle": "2023-07-30T07:20:04.210708Z",
     "shell.execute_reply": "2023-07-30T07:20:04.209874Z",
     "shell.execute_reply.started": "2023-07-30T07:20:04.201627Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x7f78287a6cb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 700\n",
    "pad_token_id = vocab.to_indices(\"[PAD]\")\n",
    "def convert_example(example, vocab):\n",
    "    _tokens = jieba.lcut(example['text'], use_paddle=True)\n",
    "    input_ids = vocab.to_indices(_tokens)\n",
    "    input_ids = input_ids[:max_len]\n",
    "    input_ids = input_ids + [pad_token_id] * (max_len - len(input_ids))\n",
    "    return input_ids, example['label']\n",
    "\n",
    "trans_func = partial(convert_example, vocab=vocab)\n",
    "train_ds.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444be668-757b-4b78-9f67-c75939d1def1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:20:04.258730Z",
     "iopub.status.busy": "2023-07-30T07:20:04.258522Z",
     "iopub.status.idle": "2023-07-30T07:20:04.488907Z",
     "shell.execute_reply": "2023-07-30T07:20:04.450903Z",
     "shell.execute_reply.started": "2023-07-30T07:20:04.258713Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "batchify_fn = lambda samples, fn=Tuple([\n",
    "    Stack(dtype=\"int64\"),\n",
    "    Stack(dtype=\"int64\")\n",
    "]): fn(samples)\n",
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = paddle.io.DataLoader(dataset=train_ds, batch_sampler=batch_sampler, collate_fn=batchify_fn, return_list=True)\n",
    "items = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a5fb3-ad54-4deb-a3fb-5bfe2162bc39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lens = []\n",
    "# for input_ids, label in train_ds:\n",
    "#     lens.append(len(input_ids))\n",
    "\n",
    "# df = pd.DataFrame(lens, columns=['len'])\n",
    "# df.describe(percentiles=[0.5, 0.999]) #700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f4f7f0-769f-4452-84ed-57d8adcf9141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:20:04.837798Z",
     "iopub.status.busy": "2023-07-30T07:20:04.837139Z",
     "iopub.status.idle": "2023-07-30T07:20:04.848639Z",
     "shell.execute_reply": "2023-07-30T07:20:04.847945Z",
     "shell.execute_reply.started": "2023-07-30T07:20:04.837774Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as nlp\n",
    "\n",
    "class TextCNN(nn.Layer):\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                num_classes,\n",
    "                emb_dim=128,\n",
    "                padding_idx=0,\n",
    "                num_filter=128,\n",
    "                ngram_filter_sizes=(3, 4, 5),\n",
    "                fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "        # 卷积层参数分别为词表长度、词嵌入维度\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
    "        self.encoder = nlp.seq2vec.CNNEncoder(emb_dim=emb_dim, num_filter=num_filter, ngram_filter_sizes=ngram_filter_sizes)\n",
    "        self.fc = nn.Linear(self.encoder.get_output_dim(), fc_hidden_size)\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, len(ngram_filter_sizes)*num_filter)\n",
    "        encoder_out = self.encoder(embedded_text)\n",
    "        encoder_out = paddle.tanh(encoder_out)\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = self.fc(encoder_out)\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        return logits\n",
    "\n",
    "model = TextCNN(vocab_size=vocab_size, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2852c049-599e-42f1-9e22-c41c7eb6902f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:20:13.067549Z",
     "iopub.status.busy": "2023-07-30T07:20:13.066867Z",
     "iopub.status.idle": "2023-07-30T07:22:03.798896Z",
     "shell.execute_reply": "2023-07-30T07:22:03.797917Z",
     "shell.execute_reply.started": "2023-07-30T07:20:13.067510Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 10: loss:0.67402, acc:0.53672\r\n",
      "epoch 1, step 20: loss:0.61313, acc:0.63516\r\n",
      "epoch 1, step 30: loss:0.52036, acc:0.67370\r\n",
      "epoch 1, step 40: loss:0.46896, acc:0.69727\r\n",
      "epoch 1, step 50: loss:0.35472, acc:0.72828\r\n",
      "epoch 1, step 60: loss:0.37869, acc:0.75404\r\n",
      "epoch 1, step 70: loss:0.37543, acc:0.77009\r\n",
      "epoch 2, step 5: loss:0.16675, acc:0.78857\r\n",
      "epoch 2, step 15: loss:0.22088, acc:0.80451\r\n",
      "epoch 2, step 25: loss:0.07682, acc:0.81828\r\n",
      "epoch 2, step 35: loss:0.16649, acc:0.83040\r\n",
      "epoch 2, step 45: loss:0.16672, acc:0.84017\r\n",
      "epoch 2, step 55: loss:0.12934, acc:0.84820\r\n",
      "epoch 2, step 65: loss:0.12369, acc:0.85541\r\n",
      "epoch 2, step 75: loss:0.12018, acc:0.86182\r\n",
      "epoch 3, step 10: loss:0.07339, acc:0.86987\r\n",
      "epoch 3, step 20: loss:0.01983, acc:0.87684\r\n",
      "epoch 3, step 30: loss:0.03650, acc:0.88303\r\n",
      "epoch 3, step 40: loss:0.03005, acc:0.88869\r\n",
      "epoch 3, step 50: loss:0.01464, acc:0.89371\r\n",
      "epoch 3, step 60: loss:0.06316, acc:0.89795\r\n",
      "epoch 3, step 70: loss:0.11069, acc:0.90206\r\n",
      "epoch 4, step 5: loss:0.00991, acc:0.90601\r\n",
      "epoch 4, step 15: loss:0.07220, acc:0.90977\r\n",
      "epoch 4, step 25: loss:0.00640, acc:0.91319\r\n",
      "epoch 4, step 35: loss:0.05383, acc:0.91629\r\n",
      "epoch 4, step 45: loss:0.01006, acc:0.91924\r\n",
      "epoch 4, step 55: loss:0.01363, acc:0.92193\r\n",
      "epoch 4, step 65: loss:0.01695, acc:0.92435\r\n",
      "epoch 4, step 75: loss:0.00534, acc:0.92669\r\n",
      "epoch 5, step 10: loss:0.08560, acc:0.92896\r\n",
      "epoch 5, step 20: loss:0.07929, acc:0.93105\r\n",
      "epoch 5, step 30: loss:0.00722, acc:0.93305\r\n",
      "epoch 5, step 40: loss:0.00303, acc:0.93486\r\n",
      "epoch 5, step 50: loss:0.04935, acc:0.93650\r\n",
      "epoch 5, step 60: loss:0.08765, acc:0.93806\r\n",
      "epoch 5, step 70: loss:0.00439, acc:0.93957\r\n",
      "epoch 6, step 5: loss:0.01301, acc:0.94108\r\n",
      "epoch 6, step 15: loss:0.00346, acc:0.94247\r\n",
      "epoch 6, step 25: loss:0.01964, acc:0.94375\r\n",
      "epoch 6, step 35: loss:0.06436, acc:0.94497\r\n",
      "epoch 6, step 45: loss:0.00305, acc:0.94621\r\n",
      "epoch 6, step 55: loss:0.00701, acc:0.94738\r\n",
      "epoch 6, step 65: loss:0.00375, acc:0.94853\r\n",
      "epoch 6, step 75: loss:0.08779, acc:0.94946\r\n",
      "epoch 7, step 10: loss:0.02429, acc:0.95049\r\n",
      "epoch 7, step 20: loss:0.05382, acc:0.95146\r\n",
      "epoch 7, step 30: loss:0.06079, acc:0.95236\r\n",
      "epoch 7, step 40: loss:0.00421, acc:0.95319\r\n",
      "epoch 7, step 50: loss:0.00410, acc:0.95403\r\n",
      "epoch 7, step 60: loss:0.00797, acc:0.95481\r\n",
      "epoch 7, step 70: loss:0.00275, acc:0.95557\r\n",
      "epoch 8, step 5: loss:0.01210, acc:0.95632\r\n",
      "epoch 8, step 15: loss:0.00327, acc:0.95706\r\n",
      "epoch 8, step 25: loss:0.00530, acc:0.95778\r\n",
      "epoch 8, step 35: loss:0.00406, acc:0.95848\r\n",
      "epoch 8, step 45: loss:0.03841, acc:0.95911\r\n",
      "epoch 8, step 55: loss:0.00456, acc:0.95973\r\n",
      "epoch 8, step 65: loss:0.00610, acc:0.96030\r\n",
      "epoch 8, step 75: loss:0.05165, acc:0.96089\r\n",
      "epoch 9, step 10: loss:0.00482, acc:0.96150\r\n",
      "epoch 9, step 20: loss:0.03388, acc:0.96206\r\n",
      "epoch 9, step 30: loss:0.00282, acc:0.96260\r\n",
      "epoch 9, step 40: loss:0.05776, acc:0.96305\r\n",
      "epoch 9, step 50: loss:0.00544, acc:0.96353\r\n",
      "epoch 9, step 60: loss:0.00246, acc:0.96400\r\n",
      "epoch 9, step 70: loss:0.02160, acc:0.96444\r\n",
      "epoch 10, step 5: loss:0.00234, acc:0.96494\r\n",
      "epoch 10, step 15: loss:0.00274, acc:0.96542\r\n",
      "epoch 10, step 25: loss:0.00135, acc:0.96589\r\n",
      "epoch 10, step 35: loss:0.03387, acc:0.96624\r\n",
      "epoch 10, step 45: loss:0.00200, acc:0.96666\r\n",
      "epoch 10, step 55: loss:0.00227, acc:0.96705\r\n",
      "epoch 10, step 65: loss:0.00645, acc:0.96741\r\n",
      "epoch 10, step 75: loss:0.06462, acc:0.96775\r\n",
      "epoch 11, step 10: loss:0.01059, acc:0.96811\r\n",
      "epoch 11, step 20: loss:0.10241, acc:0.96848\r\n",
      "epoch 11, step 30: loss:0.00222, acc:0.96888\r\n",
      "epoch 11, step 40: loss:0.09583, acc:0.96920\r\n",
      "epoch 11, step 50: loss:0.00719, acc:0.96954\r\n",
      "epoch 11, step 60: loss:0.00202, acc:0.96984\r\n",
      "epoch 11, step 70: loss:0.05194, acc:0.97011\r\n",
      "epoch 12, step 5: loss:0.00177, acc:0.97046\r\n",
      "epoch 12, step 15: loss:0.05019, acc:0.97077\r\n",
      "epoch 12, step 25: loss:0.00201, acc:0.97106\r\n",
      "epoch 12, step 35: loss:0.00259, acc:0.97134\r\n",
      "epoch 12, step 45: loss:0.00327, acc:0.97166\r\n",
      "epoch 12, step 55: loss:0.00406, acc:0.97192\r\n",
      "epoch 12, step 65: loss:0.01946, acc:0.97218\r\n",
      "epoch 12, step 75: loss:0.02787, acc:0.97242\r\n",
      "epoch 13, step 10: loss:0.00213, acc:0.97271\r\n",
      "epoch 13, step 20: loss:0.02840, acc:0.97298\r\n",
      "epoch 13, step 30: loss:0.00314, acc:0.97324\r\n",
      "epoch 13, step 40: loss:0.04499, acc:0.97350\r\n",
      "epoch 13, step 50: loss:0.00902, acc:0.97371\r\n",
      "epoch 13, step 60: loss:0.00925, acc:0.97392\r\n",
      "epoch 13, step 70: loss:0.00217, acc:0.97415\r\n",
      "epoch 14, step 5: loss:0.00231, acc:0.97438\r\n",
      "epoch 14, step 15: loss:0.03098, acc:0.97459\r\n",
      "epoch 14, step 25: loss:0.00181, acc:0.97481\r\n",
      "epoch 14, step 35: loss:0.00224, acc:0.97504\r\n",
      "epoch 14, step 45: loss:0.02846, acc:0.97522\r\n",
      "epoch 14, step 55: loss:0.04002, acc:0.97541\r\n",
      "epoch 14, step 65: loss:0.05764, acc:0.97559\r\n",
      "epoch 14, step 75: loss:0.00283, acc:0.97576\r\n",
      "epoch 15, step 10: loss:0.00819, acc:0.97596\r\n",
      "epoch 15, step 20: loss:0.04457, acc:0.97610\r\n",
      "epoch 15, step 30: loss:0.00977, acc:0.97627\r\n",
      "epoch 15, step 40: loss:0.00335, acc:0.97645\r\n",
      "epoch 15, step 50: loss:0.00217, acc:0.97662\r\n",
      "epoch 15, step 60: loss:0.00114, acc:0.97678\r\n",
      "epoch 15, step 70: loss:0.00127, acc:0.97694\r\n",
      "epoch 16, step 5: loss:0.00107, acc:0.97712\r\n",
      "epoch 16, step 15: loss:0.00206, acc:0.97728\r\n",
      "epoch 16, step 25: loss:0.01986, acc:0.97746\r\n",
      "epoch 16, step 35: loss:0.00344, acc:0.97761\r\n",
      "epoch 16, step 45: loss:0.00099, acc:0.97776\r\n",
      "epoch 16, step 55: loss:0.00176, acc:0.97791\r\n",
      "epoch 16, step 65: loss:0.00255, acc:0.97803\r\n",
      "epoch 16, step 75: loss:0.02144, acc:0.97818\r\n",
      "epoch 17, step 10: loss:0.01926, acc:0.97829\r\n",
      "epoch 17, step 20: loss:0.02140, acc:0.97844\r\n",
      "epoch 17, step 30: loss:0.00160, acc:0.97858\r\n",
      "epoch 17, step 40: loss:0.00667, acc:0.97872\r\n",
      "epoch 17, step 50: loss:0.00773, acc:0.97884\r\n",
      "epoch 17, step 60: loss:0.08586, acc:0.97896\r\n",
      "epoch 17, step 70: loss:0.00172, acc:0.97910\r\n",
      "epoch 18, step 5: loss:0.00308, acc:0.97922\r\n",
      "epoch 18, step 15: loss:0.00243, acc:0.97935\r\n",
      "epoch 18, step 25: loss:0.01835, acc:0.97948\r\n",
      "epoch 18, step 35: loss:0.00274, acc:0.97962\r\n",
      "epoch 18, step 45: loss:0.00206, acc:0.97975\r\n",
      "epoch 18, step 55: loss:0.02321, acc:0.97987\r\n",
      "epoch 18, step 65: loss:0.03102, acc:0.97994\r\n",
      "epoch 18, step 75: loss:0.00280, acc:0.98007\r\n",
      "epoch 19, step 10: loss:0.01041, acc:0.98019\r\n",
      "epoch 19, step 20: loss:0.00162, acc:0.98032\r\n",
      "epoch 19, step 30: loss:0.09779, acc:0.98043\r\n",
      "epoch 19, step 40: loss:0.00773, acc:0.98053\r\n",
      "epoch 19, step 50: loss:0.00139, acc:0.98066\r\n",
      "epoch 19, step 60: loss:0.02804, acc:0.98076\r\n",
      "epoch 19, step 70: loss:0.00182, acc:0.98085\r\n",
      "epoch 20, step 5: loss:0.01421, acc:0.98095\r\n",
      "epoch 20, step 15: loss:0.00982, acc:0.98105\r\n",
      "epoch 20, step 25: loss:0.00136, acc:0.98115\r\n",
      "epoch 20, step 35: loss:0.00141, acc:0.98126\r\n",
      "epoch 20, step 45: loss:0.03419, acc:0.98135\r\n",
      "epoch 20, step 55: loss:0.01458, acc:0.98146\r\n",
      "epoch 20, step 65: loss:0.00251, acc:0.98155\r\n",
      "epoch 20, step 75: loss:0.00566, acc:0.98164\r\n"
     ]
    }
   ],
   "source": [
    "# 定义 optimizer 优化器\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=0.001, parameters=model.parameters())\n",
    "# 定义 loss\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "metric = paddle.metric.Accuracy()\n",
    "# 训练\n",
    "epochs = 20\n",
    "global_step = 0\n",
    "with LogWriter(logdir=\"./logs\") as writer:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_dataloader, start=1):\n",
    "            input_ids, labels = batch\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            # 预测分类概率\n",
    "            correct = metric.compute(logits, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            writer.add_scalar(tag=\"acc\", step=global_step, value=acc)\n",
    "            # 向记录器添加一个tag为`loss`的数据\n",
    "            writer.add_scalar(tag=\"loss\", step=global_step, value=loss)\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0:\n",
    "                print(\"epoch %d, step %d: loss:%.5f, acc:%.5f\" % (epoch, step, loss, acc))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9c9e34-7dda-49e5-974d-7c1fd8caa54a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T07:27:14.281704Z",
     "iopub.status.busy": "2023-07-30T07:27:14.281005Z",
     "iopub.status.idle": "2023-07-30T07:27:14.563039Z",
     "shell.execute_reply": "2023-07-30T07:27:14.525056Z",
     "shell.execute_reply.started": "2023-07-30T07:27:14.281652Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 1.0\r\n"
     ]
    }
   ],
   "source": [
    "input_ids, labels = next(iter(train_dataloader))\r\n",
    "logits = model(input_ids)\r\n",
    "pred = paddle.argmax(logits, axis=-1)\r\n",
    "print(\"f1_score:\", f1_score(labels, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
